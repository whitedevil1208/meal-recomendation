{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\VIKRAM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\VIKRAM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\VIKRAM\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Entities: {'meal_type': 'breakfast', 'food_type': 'veg', 'person_type': 'atheletic', 'diet_type': 'high protein', 'disease_type': None}\n",
      "Recommended Food: panner salad with some brown bread,add some oats with milk and whey protein\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import nltk\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the Datasets\n",
    "df_normal = pd.read_excel(\"NLP DATASET.xlsx\")\n",
    "df_diseased = pd.read_excel(\"nlp dieseases dataset.xlsx\")\n",
    "\n",
    "# Preprocessing Function to clean text\n",
    "def preprocess_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = re.sub(r'\\W+', ' ', text.lower())\n",
    "    words = text.split()\n",
    "    words = [stemmer.stem(lemmatizer.lemmatize(word)) for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Combine columns to create 'features' for both datasets\n",
    "df_normal['features'] = (df_normal['type'] + \" \" + df_normal['meal type'] + \" \" + \n",
    "                         df_normal['person type'] + \" \" + df_normal['diet type'])\n",
    "df_normal['features'] = df_normal['features'].apply(preprocess_text)\n",
    "\n",
    "df_diseased['features'] = (df_diseased['type'] + \" \" + df_diseased['meal type'] + \n",
    "                           \" \" + df_diseased['disease type'])\n",
    "df_diseased['features'] = df_diseased['features'].apply(preprocess_text)\n",
    "\n",
    "# Fine-tuned NER model setup\n",
    "model_name = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "ner_pipeline = pipeline(\"ner\", model=model_name, tokenizer=model_name, aggregation_strategy=\"simple\")\n",
    "\n",
    "# Improved rule-based entity extraction\n",
    "def rule_based_entity_extraction(text):\n",
    "    meal_types = ['breakfast', 'lunch', 'dinner']\n",
    "    food_types = ['veg', 'non-veg']\n",
    "    person_types = ['weight gain', 'weight loss', 'atheletic', 'normal']\n",
    "    diet_types = ['low carb', 'high protein', 'normal']\n",
    "    disease_types = ['diabetes', 'hypertension', 'obesity', 'heart disease']\n",
    "\n",
    "    entities = {\n",
    "        'meal_type': None,\n",
    "        'food_type': None,\n",
    "        'person_type': None,\n",
    "        'diet_type': None,\n",
    "        'disease_type': None\n",
    "    }\n",
    "    \n",
    "    for meal in meal_types:\n",
    "        if meal in text.lower():\n",
    "            entities['meal_type'] = meal\n",
    "            break\n",
    "    \n",
    "    for food in food_types:\n",
    "        if food in text.lower():\n",
    "            entities['food_type'] = food\n",
    "            break\n",
    "    \n",
    "    for person in person_types:\n",
    "        if person in text.lower():\n",
    "            entities['person_type'] = person\n",
    "            break\n",
    "    \n",
    "    for diet in diet_types:\n",
    "        if diet in text.lower():\n",
    "            entities['diet_type'] = diet\n",
    "            break\n",
    "    \n",
    "    for disease in disease_types:\n",
    "        if disease in text.lower():\n",
    "            entities['disease_type'] = disease\n",
    "            break\n",
    "    \n",
    "    return entities\n",
    "\n",
    "# Extract entities using both rule-based and fine-tuned BERT-based NER\n",
    "def extract_entities(text):\n",
    "    bert_entities = ner_pipeline(text)\n",
    "    rule_entities = rule_based_entity_extraction(text)\n",
    "    \n",
    "    entities = {\n",
    "        'meal_type': rule_entities['meal_type'],\n",
    "        'food_type': rule_entities['food_type'],\n",
    "        'person_type': rule_entities['person_type'],\n",
    "        'diet_type': rule_entities['diet_type'],\n",
    "        'disease_type': rule_entities['disease_type'],\n",
    "    }\n",
    "    \n",
    "    return entities\n",
    "\n",
    "# Load a pre-trained sentence transformer model\n",
    "sentence_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Function to get sentence embedding\n",
    "def get_sentence_embedding(text, model):\n",
    "    return model.encode(text)\n",
    "\n",
    "# Vectorize both normal and diseased datasets using sentence embeddings\n",
    "df_normal['embedding'] = df_normal['features'].apply(lambda x: get_sentence_embedding(x, sentence_model))\n",
    "df_diseased['embedding'] = df_diseased['features'].apply(lambda x: get_sentence_embedding(x, sentence_model))\n",
    "\n",
    "# Function to recommend food based on extracted entities\n",
    "def recommend_food_based_on_entities(entities):\n",
    "    if entities['disease_type']:\n",
    "        user_input = f\"{entities['food_type']} {entities['meal_type']} {entities['disease_type']}\"\n",
    "        user_embedding = get_sentence_embedding(user_input, sentence_model)\n",
    "        similarities = cosine_similarity([user_embedding], np.stack(df_diseased['embedding'].values))\n",
    "        idx = np.argmax(similarities)\n",
    "        return df_diseased['recommend'].iloc[idx]\n",
    "    else:\n",
    "        user_input = f\"{entities['food_type']} {entities['meal_type']} {entities['person_type']} {entities['diet_type']}\"\n",
    "        user_embedding = get_sentence_embedding(user_input, sentence_model)\n",
    "        similarities = cosine_similarity([user_embedding], np.stack(df_normal['embedding'].values))\n",
    "        idx = np.argmax(similarities)\n",
    "        return df_normal['recommend'].iloc[idx]\n",
    "\n",
    "# Function to get user input and recommend food\n",
    "def get_recommendation():\n",
    "    prompt = input(\"Please describe your diet preferences: \")\n",
    "    extracted_entities = extract_entities(prompt)\n",
    "    print(f\"Extracted Entities: {extracted_entities}\")\n",
    "    recommendation = recommend_food_based_on_entities(extracted_entities)\n",
    "    print(\"Recommended Food:\", recommendation)\n",
    "\n",
    "# Run the system\n",
    "get_recommendation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nutritional Analysis:\n",
      " Energy: 75.11999999999999 kcal\n",
      "Total lipid (fat): 1.0248 g\n",
      "Fatty acids, total saturated: 0.23736 g\n",
      "Fatty acids, total monounsaturated: 0.2448 g\n",
      "Fatty acids, total polyunsaturated: 0.4128 g\n",
      "Carbohydrate, by difference: 13.392 g\n",
      "Carbohydrates (net): 12.264 g\n",
      "Fiber, total dietary: 1.128 g\n",
      "Sugars, total including NLEA: 1.5408 g\n",
      "Protein: 3.12 g\n",
      "Cholesterol: 0.0 mg\n",
      "Sodium, Na: 144.23999999999998 mg\n",
      "Calcium, Ca: 39.6 mg\n",
      "Magnesium, Mg: 14.16 mg\n",
      "Potassium, K: 53.519999999999996 mg\n",
      "Iron, Fe: 0.9815999999999999 mg\n",
      "Zinc, Zn: 0.3528 mg\n",
      "Phosphorus, P: 45.12 mg\n",
      "Vitamin A, RAE: 0.0 µg\n",
      "Vitamin C, total ascorbic acid: 0.048 mg\n",
      "Thiamin: 0.10536 mg\n",
      "Riboflavin: 0.09168 mg\n",
      "Niacin: 1.5 mg\n",
      "Vitamin B-6: 0.036719999999999996 mg\n",
      "Folate, DFE: 24.0 µg\n",
      "Folate, food: 15.6 µg\n",
      "Folic acid: 4.8 µg\n",
      "Vitamin B-12: 0.0 µg\n",
      "Vitamin D (D2 + D3): 0.0 µg\n",
      "Vitamin E (alpha-tocopherol): 0.0576 mg\n",
      "Vitamin K (phylloquinone): 1.3679999999999999 µg\n",
      "Water: 5.808 g\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Set your Edamam API credentials\n",
    "EDAMAM_APP_ID = '2a98c6a4'  # Replace with your Edamam APP ID\n",
    "EDAMAM_APP_KEY = '75704984fc122dc3153ae7a943f3cb56'  # Replace with your Edamam APP KEY\n",
    "\n",
    "# Function to get nutritional information from Edamam API\n",
    "def get_nutritional_info(food_item):\n",
    "    url = f\"https://api.edamam.com/api/nutrition-data?app_id={EDAMAM_APP_ID}&app_key={EDAMAM_APP_KEY}&nutrition-type=logging&ingr={food_item}\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to format nutritional information for output\n",
    "def format_nutrition_info(nutrition_data):\n",
    "    if nutrition_data:\n",
    "        nutrients = nutrition_data.get('totalNutrients', {})\n",
    "        formatted_info = []\n",
    "        for nutrient, details in nutrients.items():\n",
    "            formatted_info.append(f\"{details['label']}: {details['quantity']} {details['unit']}\")\n",
    "        return \"\\n\".join(formatted_info)\n",
    "    return \"Nutritional information not available.\"\n",
    "\n",
    "# Function to get user input and perform nutritional analysis\n",
    "def get_nutritional_analysis():\n",
    "    # Get user input for food item\n",
    "    food_item = input(\"Enter the food item you want nutritional information for: \")\n",
    "\n",
    "    # Get nutritional analysis for the food item\n",
    "    nutrition = get_nutritional_info(food_item)\n",
    "    nutritional_analysis = format_nutrition_info(nutrition)\n",
    "    print(\"Nutritional Analysis:\\n\", nutritional_analysis)\n",
    "\n",
    "# Run the system\n",
    "get_nutritional_analysis()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI,HTTPException\n",
    "import openai\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "@app.post(\"/analyze/\")\n",
    "async def analyze_input(user_input: str):\n",
    "    prompt = f\"Given the description: '{user_input}', suggest diet preferences.\"\n",
    "    \n",
    "    try:\n",
    "        # Call OpenAI API to analyze user input\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        # Extract the response text\n",
    "        openai_response = response.choices[0].message['content'].strip()\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error from OpenAI: {str(e)}\")\n",
    "\n",
    "    return {\n",
    "        \"analysis\": openai_response\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diet Recommendation: It is difficult to suggest specific diet preferences without more information about the individual's personal health goals, dietary restrictions, allergies, and cultural background. However, some general diet preferences that could be considered based on the limited information provided are:\n",
      "\n",
      "1. Plant-based diet: A diet focused on whole, plant-based foods such as fruits, vegetables, whole grains, nuts, and seeds.\n",
      "2. Low-carb diet: A diet that limits the intake of carbohydrates and emphasizes protein and healthy fats.\n",
      "3. Mediterranean diet: A diet rich in fruits, vegetables, whole grains, lean proteins, and healthy fats like olive oil.\n",
      "4. Gluten-free diet: A diet that eliminates gluten-containing foods for individuals with celiac disease or gluten sensitivity.\n",
      "5. Paleo diet: A diet that focuses on eating foods that mimic what our ancestors ate, such as lean meats, fruits, vegetables, nuts, and seeds.\n",
      "\n",
      "Ultimately, the best diet preferences will depend on the individual's unique needs and goals, so it is recommended to consult with a healthcare provider or registered dietitian for personalized recommendations.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def get_diet_recommendation(prompt):\n",
    "    openai_prompt = f\"Given the description: '{prompt}', suggest diet preferences.\"\n",
    "\n",
    "    try:\n",
    "        # Call OpenAI API to analyze user input\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": openai_prompt}]\n",
    "        )\n",
    "        # Extract the response text\n",
    "        openai_response = response.choices[0].message['content'].strip()\n",
    "        return openai_response\n",
    "    except Exception as e:\n",
    "        return f\"Error from OpenAI: {str(e)}\"\n",
    "\n",
    "# Get user input\n",
    "user_prompt = input(\"Enter your prompt: \")\n",
    "\n",
    "# Get and print the recommendation\n",
    "recommendation = get_diet_recommendation(user_prompt)\n",
    "print(\"Diet Recommendation:\", recommendation)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
