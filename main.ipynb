{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "meal recommendation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normal = pd.read_excel(\"NLP DATASET.xlsx\")\n",
    "df_diseased = pd.read_excel(\"nlp dieseases dataset.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = re.sub(r'\\W+', ' ', text.lower())\n",
    "    words = text.split()\n",
    "    words = [stemmer.stem(lemmatizer.lemmatize(word)) for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normal['features'] = (df_normal['type'] + \" \" + df_normal['meal type'] + \" \" + \n",
    "                         df_normal['person type'] + \" \" + df_normal['diet type'])\n",
    "df_normal['features'] = df_normal['features'].apply(preprocess_text)\n",
    "\n",
    "df_diseased['features'] = (df_diseased['type'] + \" \" + df_diseased['meal type'] + \n",
    "                           \" \" + df_diseased['disease type'])\n",
    "df_diseased['features'] = df_diseased['features'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the Fine-Tuned NER Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "ner_pipeline = pipeline(\"ner\", model=model_name, tokenizer=model_name, aggregation_strategy=\"simple\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rule-Based Entity Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_based_entity_extraction(text):\n",
    "    meal_types = ['breakfast', 'lunch', 'dinner']\n",
    "    food_types = ['veg', 'non-veg']\n",
    "    person_types = ['weight gain', 'weight loss', 'atheletic', 'normal']\n",
    "    diet_types = ['low carb', 'high protein', 'normal']\n",
    "    disease_types = ['diabetes', 'hypertension', 'obesity', 'heart disease']\n",
    "\n",
    "    entities = {\n",
    "        'meal_type': None,\n",
    "        'food_type': None,\n",
    "        'person_type': None,\n",
    "        'diet_type': None,\n",
    "        'disease_type': None\n",
    "    }\n",
    "    \n",
    "    for meal in meal_types:\n",
    "        if meal in text.lower():\n",
    "            entities['meal_type'] = meal\n",
    "            break\n",
    "    \n",
    "    for food in food_types:\n",
    "        if food in text.lower():\n",
    "            entities['food_type'] = food\n",
    "            break\n",
    "    \n",
    "    for person in person_types:\n",
    "        if person in text.lower():\n",
    "            entities['person_type'] = person\n",
    "            break\n",
    "    \n",
    "    for diet in diet_types:\n",
    "        if diet in text.lower():\n",
    "            entities['diet_type'] = diet\n",
    "            break\n",
    "    \n",
    "    for disease in disease_types:\n",
    "        if disease in text.lower():\n",
    "            entities['disease_type'] = disease\n",
    "            break\n",
    "    \n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entity Extraction Combining BERT and Rule-Based Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(text):\n",
    "    bert_entities = ner_pipeline(text)\n",
    "    rule_entities = rule_based_entity_extraction(text)\n",
    "    \n",
    "    entities = {\n",
    "        'meal_type': rule_entities['meal_type'],\n",
    "        'food_type': rule_entities['food_type'],\n",
    "        'person_type': rule_entities['person_type'],\n",
    "        'diet_type': rule_entities['diet_type'],\n",
    "        'disease_type': rule_entities['disease_type'],\n",
    "    }\n",
    "    \n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Sentence Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VIKRAM\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sentence_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to Get Sentence Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embedding(text, model):\n",
    "    return model.encode(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize Both Normal and Diseased Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VIKRAM\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "df_normal['embedding'] = df_normal['features'].apply(lambda x: get_sentence_embedding(x, sentence_model))\n",
    "df_diseased['embedding'] = df_diseased['features'].apply(lambda x: get_sentence_embedding(x, sentence_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normal['embedding'] = df_normal['features'].apply(lambda x: get_sentence_embedding(x, sentence_model))\n",
    "df_diseased['embedding'] = df_diseased['features'].apply(lambda x: get_sentence_embedding(x, sentence_model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to Recommend Food Based on Extracted Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_food_based_on_entities(entities):\n",
    "    if entities['disease_type']:\n",
    "        user_input = f\"{entities['food_type']} {entities['meal_type']} {entities['disease_type']}\"\n",
    "        user_embedding = get_sentence_embedding(user_input, sentence_model)\n",
    "        similarities = cosine_similarity([user_embedding], np.stack(df_diseased['embedding'].values))\n",
    "        idx = np.argmax(similarities)\n",
    "        return df_diseased['recommend'].iloc[idx]\n",
    "    else:\n",
    "        user_input = f\"{entities['food_type']} {entities['meal_type']} {entities['person_type']} {entities['diet_type']}\"\n",
    "        user_embedding = get_sentence_embedding(user_input, sentence_model)\n",
    "        similarities = cosine_similarity([user_embedding], np.stack(df_normal['embedding'].values))\n",
    "        idx = np.argmax(similarities)\n",
    "        return df_normal['recommend'].iloc[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get recommendationn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Entities: {'meal_type': 'breakfast', 'food_type': 'veg', 'person_type': 'normal', 'diet_type': 'normal', 'disease_type': None}\n",
      "Recommended Food: idly or dosa or any type of breakfast\n"
     ]
    }
   ],
   "source": [
    "def get_recommendation():\n",
    "    prompt = input(\"Please describe your diet preferences: \")\n",
    "    extracted_entities = extract_entities(prompt)\n",
    "    print(f\"Extracted Entities: {extracted_entities}\")\n",
    "    recommendation = recommend_food_based_on_entities(extracted_entities)\n",
    "    print(\"Recommended Food:\", recommendation)\n",
    "\n",
    "\n",
    "get_recommendation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nutritional analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Set your Edamam API credentials\n",
    "EDAMAM_APP_ID = '2a98c6a4'  # Replace with your Edamam APP ID\n",
    "EDAMAM_APP_KEY = '75704984fc122dc3153ae7a943f3cb56'  # Replace with your Edamam APP KEY\n",
    "\n",
    "# Function to get nutritional information from Edamam API\n",
    "def get_nutritional_info(food_item):\n",
    "    url = f\"https://api.edamam.com/api/nutrition-data?app_id={EDAMAM_APP_ID}&app_key={EDAMAM_APP_KEY}&nutrition-type=logging&ingr={food_item}\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to format nutritional information for output\n",
    "def format_nutrition_info(nutrition_data):\n",
    "    if nutrition_data:\n",
    "        nutrients = nutrition_data.get('totalNutrients', {})\n",
    "        formatted_info = []\n",
    "        for nutrient, details in nutrients.items():\n",
    "            formatted_info.append(f\"{details['label']}: {details['quantity']} {details['unit']}\")\n",
    "        return \"\\n\".join(formatted_info)\n",
    "    return \"Nutritional information not available.\"\n",
    "\n",
    "# Function to get user input and perform nutritional analysis\n",
    "def get_nutritional_analysis():\n",
    "    # Get user input for food item\n",
    "    food_item = input(\"Enter the food item you want nutritional information for: \")\n",
    "\n",
    "    # Get nutritional analysis for the food item\n",
    "    nutrition = get_nutritional_info(food_item)\n",
    "    nutritional_analysis = format_nutrition_info(nutrition)\n",
    "    print(\"Nutritional Analysis:\\n\", nutritional_analysis)\n",
    "\n",
    "# Run the system\n",
    "get_nutritional_analysis()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\VIKRAM\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\VIKRAM\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import nltk\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "\n",
    "# Load datasets\n",
    "df_normal = pd.read_excel(\"NLP DATASET.xlsx\")\n",
    "df_diseased = pd.read_excel(\"nlp dieseases dataset.xlsx\")\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = re.sub(r'\\W+', ' ', text.lower())\n",
    "    words = text.split()\n",
    "    words = [stemmer.stem(lemmatizer.lemmatize(word)) for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Prepare features\n",
    "df_normal['features'] = (df_normal['type'] + \" \" + df_normal['meal type'] + \" \" + \n",
    "                         df_normal['person type'] + \" \" + df_normal['diet type'])\n",
    "df_normal['features'] = df_normal['features'].apply(preprocess_text)\n",
    "\n",
    "df_diseased['features'] = (df_diseased['type'] + \" \" + df_diseased['meal type'] + \n",
    "                           \" \" + df_diseased['disease type'])\n",
    "df_diseased['features'] = df_diseased['features'].apply(preprocess_text)\n",
    "\n",
    "# Load NER model\n",
    "model_name = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "ner_pipeline = pipeline(\"ner\", model=model_name, tokenizer=model_name, aggregation_strategy=\"simple\")\n",
    "\n",
    "# Rule-based entity extraction function\n",
    "def rule_based_entity_extraction(text):\n",
    "    meal_types = ['breakfast', 'lunch', 'dinner']\n",
    "    food_types = ['veg', 'non-veg']\n",
    "    person_types = ['weight gain', 'weight loss', 'athletic', 'normal']\n",
    "    diet_types = ['low carb', 'high protein', 'normal']\n",
    "    disease_types = ['diabetes', 'hypertension', 'obesity', 'heart disease']\n",
    "\n",
    "    entities = {\n",
    "        'meal_type': None,\n",
    "        'food_type': None,\n",
    "        'person_type': None,\n",
    "        'diet_type': None,\n",
    "        'disease_type': None\n",
    "    }\n",
    "    \n",
    "    for meal in meal_types:\n",
    "        if meal in text.lower():\n",
    "            entities['meal_type'] = meal\n",
    "            break\n",
    "    \n",
    "    for food in food_types:\n",
    "        if food in text.lower():\n",
    "            entities['food_type'] = food\n",
    "            break\n",
    "    \n",
    "    for person in person_types:\n",
    "        if person in text.lower():\n",
    "            entities['person_type'] = person\n",
    "            break\n",
    "    \n",
    "    for diet in diet_types:\n",
    "        if diet in text.lower():\n",
    "            entities['diet_type'] = diet\n",
    "            break\n",
    "    \n",
    "    for disease in disease_types:\n",
    "        if disease in text.lower():\n",
    "            entities['disease_type'] = disease\n",
    "            break\n",
    "    \n",
    "    return entities\n",
    "\n",
    "# Entity extraction function\n",
    "def extract_entities(text):\n",
    "    bert_entities = ner_pipeline(text)\n",
    "    rule_entities = rule_based_entity_extraction(text)\n",
    "    \n",
    "    entities = {\n",
    "        'meal_type': rule_entities['meal_type'],\n",
    "        'food_type': rule_entities['food_type'],\n",
    "        'person_type': rule_entities['person_type'],\n",
    "        'diet_type': rule_entities['diet_type'],\n",
    "        'disease_type': rule_entities['disease_type'],\n",
    "    }\n",
    "    \n",
    "    return entities\n",
    "\n",
    "# Sentence embedding model\n",
    "sentence_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "def get_sentence_embedding(text, model):\n",
    "    return model.encode(text)\n",
    "\n",
    "df_normal['embedding'] = df_normal['features'].apply(lambda x: get_sentence_embedding(x, sentence_model))\n",
    "df_diseased['embedding'] = df_diseased['features'].apply(lambda x: get_sentence_embedding(x, sentence_model))\n",
    "\n",
    "# Food recommendation function\n",
    "def recommend_food_based_on_entities(entities):\n",
    "    if entities['disease_type']:\n",
    "        user_input = f\"{entities['food_type']} {entities['meal_type']} {entities['disease_type']}\"\n",
    "        user_embedding = get_sentence_embedding(user_input, sentence_model)\n",
    "        similarities = cosine_similarity([user_embedding], np.stack(df_diseased['embedding'].values))\n",
    "        idx = np.argmax(similarities)\n",
    "        return df_diseased['recommend'].iloc[idx]\n",
    "    else:\n",
    "        user_input = f\"{entities['food_type']} {entities['meal_type']} {entities['person_type']} {entities['diet_type']}\"\n",
    "        user_embedding = get_sentence_embedding(user_input, sentence_model)\n",
    "        similarities = cosine_similarity([user_embedding], np.stack(df_normal['embedding'].values))\n",
    "        idx = np.argmax(similarities)\n",
    "        return df_normal['recommend'].iloc[idx]\n",
    "\n",
    "# Edamam API credentials\n",
    "EDAMAM_APP_ID = '2a98c6a4'  # Replace with your Edamam APP ID\n",
    "EDAMAM_APP_KEY = '75704984fc122dc3153ae7a943f3cb56'  # Replace with your Edamam APP KEY\n",
    "\n",
    "# Function to get nutritional information from Edamam API\n",
    "def get_nutritional_info(food_item):\n",
    "    url = f\"https://api.edamam.com/api/nutrition-data?app_id={EDAMAM_APP_ID}&app_key={EDAMAM_APP_KEY}&nutrition-type=logging&ingr={food_item}\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to format nutritional information for output\n",
    "def format_nutrition_info(nutrition_data):\n",
    "    if nutrition_data:\n",
    "        nutrients = nutrition_data.get('totalNutrients', {})\n",
    "        formatted_info = []\n",
    "        for nutrient, details in nutrients.items():\n",
    "            formatted_info.append(f\"{details['label']}: {details['quantity']} {details['unit']}\")\n",
    "        return \"\\n\".join(formatted_info)\n",
    "    return \"Nutritional information not available.\"\n",
    "\n",
    "# GUI class\n",
    "class MealRecommendationApp:\n",
    "    def __init__(self, master):\n",
    "        self.master = master\n",
    "        master.title(\"Meal Recommendation System\")\n",
    "\n",
    "        self.label = tk.Label(master, text=\"Describe your diet preferences:\")\n",
    "        self.label.pack()\n",
    "\n",
    "        self.entry = tk.Entry(master, width=50)\n",
    "        self.entry.pack()\n",
    "\n",
    "        self.recommend_button = tk.Button(master, text=\"Get Food Recommendation\", command=self.get_recommendation)\n",
    "        self.recommend_button.pack()\n",
    "\n",
    "        self.nutrition_button = tk.Button(master, text=\"Get Nutritional Analysis\", command=self.get_nutritional_analysis)\n",
    "        self.nutrition_button.pack()\n",
    "\n",
    "    def get_recommendation(self):\n",
    "        prompt = self.entry.get()\n",
    "        extracted_entities = extract_entities(prompt)\n",
    "        recommendation = recommend_food_based_on_entities(extracted_entities)\n",
    "        messagebox.showinfo(\"Recommended Food\", recommendation)\n",
    "\n",
    "    def get_nutritional_analysis(self):\n",
    "        food_item = self.entry.get()\n",
    "        nutrition = get_nutritional_info(food_item)\n",
    "        nutritional_analysis = format_nutrition_info(nutrition)\n",
    "        messagebox.showinfo(\"Nutritional Analysis\", nutritional_analysis)\n",
    "\n",
    "# Run the application\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = MealRecommendationApp(root)\n",
    "    root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
